@inproceedings{food101,
  title = {Food-101 -- Mining Discriminative Components with Random Forests},
  author = {Bossard, Lukas and Guillaumin, Matthieu and Van Gool, Luc},
  booktitle = {European Conference on Computer Vision},
  year = {2014}
}

@incollection{srinivasFullgradientRepresentationNeural2019,
  title = {Full-Gradient Representation for Neural Network Visualization},
  booktitle = {Proceedings of the 33rd {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Srinivas, Suraj and Fleuret, Fran{\c c}ois},
  year = {2019},
  month = dec,
  number = {371},
  pages = {4124--4133},
  publisher = {Curran Associates Inc.},
  address = {Red Hook, NY, USA},
  urldate = {2024-03-26},
  abstract = {We introduce a new tool for interpreting neural net responses, namely full-gradients, which decomposes the neural net response into input sensitivity and per-neuron sensitivity components. This is the first proposed representation which satisfies two key properties: completeness and weak dependence, which provably cannot be satisfied by any saliency map-based interpretability method. For convolutional nets, we also propose an approximate saliency map representation, called FullGrad, obtained by aggregating the full-gradient components. We experimentally evaluate the usefulness of FullGrad in explaining model behaviour with two quantitative tests: pixel perturbation and remove-and-retrain. Our experiments reveal that our method explains model behavior correctly, and more comprehensively, than other methods in the literature. Visual inspection also reveals that our saliency maps are sharper and more tightly confined to object regions than other methods.},
  file = {/home/arvid/snap/zotero-snap/common/Zotero/storage/AKFK5RXH/Srinivas and Fleuret - 2019 - Full-gradient representation for neural network vi.pdf}
}

@inproceedings{gradcamp,
	doi = {10.1109/wacv.2018.00097},
	year = 2018,
	month = {mar},
	publisher = {{IEEE}},
  	author = {Aditya Chattopadhay and Anirban Sarkar and Prantik Howlader and Vineeth N Balasubramanian},
  	title = {{Grad-{CAM}++: Generalized Gradient-Based Visual Explanations for Deep Convolutional Networks}},
  	booktitle = {2018 {IEEE} Winter Conference on Applications of Computer Vision ({WACV})}
}

@misc{hirescam,
      title= {Use HiResCAM instead of Grad-CAM for faithful explanations of convolutional neural networks}, 
      author= {Rachel Lea Draelos and Lawrence Carin},
      year= {2021},
      eprint= {2011.08891},
      archivePrefix= {arXiv},
      primaryClass= {eess.IV}
}

@misc{nlplstmexplaining,
      title={Explaining Recurrent Neural Network Predictions in Sentiment Analysis}, 
      author={Leila Arras and Grégoire Montavon and Klaus-Robert Müller and Wojciech Samek},
      year={2017},
      eprint={1706.07206},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{gradcam,
	doi = {10.1007/s11263-019-01228-7},
  
	url = {https://doi.org/10.1007%2Fs11263-019-01228-7},
  
	year = 2019,
	month = {oct},
  
	publisher = {Springer Science and Business Media {LLC}
},
  
	volume = {128},
  
	number = {2},
  
	pages = {336--359},
  
	author = {Ramprasaath R. Selvaraju and Michael Cogswell and Abhishek Das and Ramakrishna Vedantam and Devi Parikh and Dhruv Batra},
  
	title = {Grad-{CAM}: Visual Explanations from Deep Networks via Gradient-Based Localization},
  
	journal = {International Journal of Computer Vision}
}

@article{xgradcam,
  title={{Axiom-based Grad-CAM: Towards Accurate Visualization and Explanation of CNNs}},
  author={Ruigang Fu and Qingyong Hu and Xiaohu Dong and Yulan Guo and Yinghui Gao and Biao Li},
  journal={ArXiv},
  year={2020},
  volume={abs/2008.02312},
  url={https://api.semanticscholar.org/CorpusID:221006223}
}

@inproceedings{
wang2022why,
title={{{\textquotedblleft}Why Not Other Classes?{\textquotedblright}: Towards Class-Contrastive Back-Propagation Explanations}},
author={Yipei Wang and Xiaoqian Wang},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=X5eFS09r9hm}
}

@article{multilabelmodel,
  author       = {Zhao{-}Min Chen and
                  Xiu{-}Shen Wei and
                  Peng Wang and
                  Yanwen Guo},
  title        = {Multi-Label Image Recognition with Graph Convolutional Networks},
  journal      = {CoRR},
  volume       = {abs/1904.03582},
  year         = {2019},
  url          = {http://arxiv.org/abs/1904.03582},
  eprinttype    = {arXiv},
  eprint       = {1904.03582},
  timestamp    = {Mon, 16 Nov 2020 13:56:02 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1904-03582.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{baselinesimpact,
author = {Sturmfels, Pascal and Lundberg, Scott and Lee, Su-In},
year = {2020},
month = {01},
pages = {},
title = {{Visualizing the Impact of Feature Attribution Baselines}},
volume = {5},
journal = {Distill},
doi = {10.23915/distill.00022}
}

@misc{mscoco,
  abstract = {We present a new dataset with the goal of advancing the state-of-the-art in
object recognition by placing the question of object recognition in the context
of the broader question of scene understanding. This is achieved by gathering
images of complex everyday scenes containing common objects in their natural
context. Objects are labeled using per-instance segmentations to aid in precise
object localization. Our dataset contains photos of 91 objects types that would
be easily recognizable by a 4 year old. With a total of 2.5 million labeled
instances in 328k images, the creation of our dataset drew upon extensive crowd
worker involvement via novel user interfaces for category detection, instance
spotting and instance segmentation. We present a detailed statistical analysis
of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide
baseline performance analysis for bounding box and segmentation detection
results using a Deformable Parts Model.},
  added-at = {2020-06-07T20:25:18.000+0200},
  author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Bourdev, Lubomir and Girshick, Ross and Hays, James and Perona, Pietro and Ramanan, Deva and Zitnick, C. Lawrence and Dollár, Piotr},
  biburl = {https://www.bibsonomy.org/bibtex/2f4ab9f41677ee189a8cbc5a92cc0dc74/jan.hofmann1},
  description = {Microsoft COCO: Common Objects in Context},
  interhash = {a3a26c6fe173264a6b812e3b7b4119bd},
  intrahash = {f4ab9f41677ee189a8cbc5a92cc0dc74},
  keywords = {thema:pyramid_scene_parsing},
  note = {cite arxiv:1405.0312Comment: 1) updated annotation pipeline description and figures; 2) added new  section describing datasets splits; 3) updated author list},
  timestamp = {2020-06-07T20:25:18.000+0200},
  title = {Microsoft COCO: Common Objects in Context},
  url = {http://arxiv.org/abs/1405.0312},
  year = 2014
}


 @book{cub200, title={The Caltech-UCSD Birds-200-2011 Dataset}, abstractNote={CUB-200-2011 is an extended version of CUB-200 [7], a challenging dataset of 200 bird species. The extended version roughly doubles the number of images per category and adds new part localization annotations. All images are annotated with bounding boxes, part locations, and at- tribute labels. Images and annotations were filtered by mul- tiple users of Mechanical Turk. We introduce benchmarks and baseline experiments for multi-class categorization and part localization.}, institution={California Institute of Technology}, author={Wah, Catherine and Branson, Steve and Welinder, Peter and Perona, Pietro and Belongie, Serge}, year={2011}, month={Jul} }

@InProceedings{Nilsback08,
  author       = "Maria-Elena Nilsback and Andrew Zisserman",
  title        = "Automated Flower Classification over a Large Number of Classes",
  booktitle    = "Indian Conference on Computer Vision, Graphics and Image Processing",
  month        = "Dec",
  year         = "2008",
}

@inproceedings{Bossard2014Food101M,
author="Bossard, Lukas
and Guillaumin, Matthieu
and Van Gool, Luc",
title="Food-101 -- Mining Discriminative Components with Random Forests",
booktitle={European Conference on Computer Vision},
year="2014",
pages="446--461",
}

@misc{ coco_dataset,
    title = { COCO Dataset Dataset },
    type = { Open Source Dataset },
    author = { Microsoft },
    howpublished = { \url{ https://universe.roboflow.com/microsoft/coco } },
    url = { https://universe.roboflow.com/microsoft/coco },
    journal = { Roboflow Universe },
    publisher = { Roboflow },
    year = { 2023 },
    month = { oct },
    note = { visited on 2023-10-26 },
}

@inproceedings{ML-GCN_CVPR_2019,
author = {Zhao-Min Chen and Xiu-Shen Wei and Peng Wang and Yanwen Guo},
title = {{Multi-Label Image Recognition with Graph Convolutional Networks}},
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
year = {2019}
}


@inproceedings{gomez2022metrics,
  title={Metrics for saliency map evaluation of deep learning explanation methods},
  author={Gomez, Tristan and Fr{\'e}our, Thomas and Mouch{\`e}re, Harold},
  booktitle={International Conference on Pattern Recognition and Artificial Intelligence},
  pages={84--95},
  year={2022},
  organization={Springer}
}


@inproceedings{ILSVRC2012,
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Kai Li and Li Fei-Fei},
  booktitle={2009 IEEE Conference on Computer Vision and Pattern Recognition}, 
  title={{ImageNet: A large-scale hierarchical image database}}, 
  year={2009},
  volume={},
  number={},
  pages={248-255},
  doi={10.1109/CVPR.2009.5206848}}

@article{vgg16,
  title={{Very Deep Convolutional Networks for Large-Scale Image Recognition}},
  author={Karen Simonyan and Andrew Zisserman},
  journal={CoRR},
  year={2014},
  volume={abs/1409.1556},
  url={https://api.semanticscholar.org/CorpusID:14124313}
}

@pre{alexnet,
      title={One weird trick for parallelizing convolutional neural networks}, 
      author={Alex Krizhevsky},
      year={2014},
      eprint={1404.5997},
      archivePrefix={arXiv},
      primaryClass={cs.NE}
}


@article{visual-transformer,
	author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
	title = {{An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale}},
	journal = {arXiv},
	year = {2020},
	month = oct,
	eprint = {2010.11929},
	doi = {10.48550/arXiv.2010.11929}
}


@article{deit-visual-transformer,
	author = {Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and J{\ifmmode\acute{e}\else\'{e}\fi}gou, Herv{\ifmmode\acute{e}\else\'{e}\fi}},
	title = {{Training data-efficient image transformers {\&} distillation through attention}},
	journal = {arXiv},
	year = {2020},
	month = dec,
	eprint = {2012.12877},
	doi = {10.48550/arXiv.2012.12877}
}


@misc{vit-model-pytorch,
	title = {{vit{$\_$}b{$\_$}16 {\ifmmode---\else\textemdash\fi} Torchvision main documentation}},
	year = {2023},
	month = dec,
	note = {[Online; accessed 14. Dec. 2023]},
	url = {https://pytorch.org/vision/main/models/generated/torchvision.models.vit_b_16.html}
}

@misc{gradcam-vit-backup,
	title = {{pytorch-grad-cam/tutorials/vision{$\_$}transformers.md at master {$\cdot$} jacobgil/pytorch-grad-cam}},
	journal = {GitHub},
	year = {2023},
	month = dec,
	note = {[Online; accessed 15. Dec. 2023]},
	url = {https://github.com/jacobgil/pytorch-grad-cam/blob/master/tutorials/vision_transformers.md}
}

@misc{gradcam-vit,
  title={{PyTorch library for CAM methods}},
  author={Jacob Gildenblat and contributors},
  year={2021},
  publisher={GitHub},
  howpublished={\url{https://github.com/jacobgil/pytorch-grad-cam}},
}

@article{Quantifying-Attention-Flow,
	author = {Abnar, Samira and Zuidema, Willem},
	title = {{Quantifying Attention Flow in Transformers}},
	journal = {arXiv},
	year = {2020},
	month = may,
	eprint = {2005.00928},
	doi = {10.48550/arXiv.2005.00928}
}

@misc{blog-grad-rollout,
	title = {{Exploring Explainability for Vision Transformers}},
	journal = {Jacob Gildenblat},
        author = {Jacob Gildenblat},
	year = {2020},
	month = dec,
	note = {[Online; accessed 15. Dec. 2023]},
	url = {https://jacobgil.github.io/deeplearning/vision-transformer-explainability#gradient-attention-rollout-for-class-specific-explainability}
}

@article{Chefer2020Dec,
	author = {Chefer, Hila and Gur, Shir and Wolf, Lior},
	title = {{Transformer Interpretability Beyond Attention Visualization}},
	journal = {arXiv},
	year = {2020},
	month = dec,
	eprint = {2012.09838},
	doi = {10.48550/arXiv.2012.09838}
}



