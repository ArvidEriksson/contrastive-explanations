{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64ea61c-dff0-4e5f-a963-e7ceee2f2c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\" # If using GPU then use mixed precision training.\n",
    "\n",
    "# Load ImageNet\n",
    "\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "normalization = transforms.Compose([\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "data_dir = '../ILSVRC2012'\n",
    "val_dataset = datasets.ImageNet(root=data_dir, split='val', transform=data_transforms)\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "dataset_sizes = len(val_dataset)\n",
    "class_names = val_dataset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a192f26d-3625-4e2c-86ed-7a09a51f85e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "available_models = {\n",
    "    'resnet18' : torchvision.models.resnet18,\n",
    "    'alexnet' : torchvision.models.alexnet,\n",
    "    'googlenet' : torchvision.models.googlenet,\n",
    "    'mobilenet_v3_small' : torchvision.models.mobilenet_v3_small,\n",
    "    'mobilenet_v3_large' : torchvision.models.mobilenet_v3_large,\n",
    "    'mnasnet1_0' : torchvision.models.mnasnet1_0,\n",
    "    'vgg16' : torchvision.models.vgg16,\n",
    "    'efficientnet_b1' : torchvision.models.efficientnet_b1,\n",
    "    'densenet161' : torchvision.models.densenet161\n",
    "}\n",
    "\n",
    "def get_model(model_type):\n",
    "\n",
    "    # Check if the specified model name is valid\n",
    "    if model_type not in available_models:\n",
    "        raise ValueError(f\"Invalid model name. Available models: {list(available_models.keys())}\")\n",
    "    # Load the model with pre-trained weights\n",
    "    model = available_models[model_type](weights=\"DEFAULT\").to(device)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2c31b9-c414-4cc9-b0ec-a011be666ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_explanation(model, inputs_no_norm, targets, explanation_mode):\n",
    "    model.eval()\n",
    "\n",
    "    inputs = normalization(inputs_no_norm.clone().detach())\n",
    "\n",
    "    inputs.requires_grad = True\n",
    "    model.zero_grad()\n",
    "\n",
    "    outputs = model(inputs)\n",
    "\n",
    "    if explanation_mode == 'normal':\n",
    "        loss = outputs[torch.arange(outputs.size(0)), targets ].sum()\n",
    "    elif explanation_mode == 'max':\n",
    "        probs, preds = torch.topk(outputs, 2)\n",
    "        correct_classifications = preds[:, 0] == targets\n",
    "        wrong_classifications = preds[:, 0] != targets\n",
    "        max_index_not_target = correct_classifications * preds[:, 1] + wrong_classifications * preds[:, 0]\n",
    "        loss = ( outputs[torch.arange(outputs.size(0)), targets ] - outputs[torch.arange(outputs.size(0)), max_index_not_target ] ).sum()\n",
    "    elif explanation_mode == 'weighted':\n",
    "        outputs = torch.softmax(outputs, dim=1)\n",
    "        loss = outputs[torch.arange(outputs.size(0)), targets ].sum()\n",
    "    elif explanation_mode == 'mean':\n",
    "        weights = -torch.ones(outputs.shape).to(outputs.device)/999\n",
    "        weights[range(len(weights)), targets] = 1\n",
    "        loss = (weights * outputs).sum()\n",
    "\n",
    "    loss.backward()\n",
    "    gradients = inputs.grad.clone().detach()\n",
    "    explanations = gradients\n",
    "    inputs.requires_grad = False\n",
    "\n",
    "    return explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2bb5fe-76c5-4e21-94cd-6e2ddf4ab79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model('vgg16')\n",
    "\n",
    "logit_explanation_norm = []\n",
    "\n",
    "for i, batch in enumerate(iter(dataloader)):\n",
    "    if i >= 1000:\n",
    "        break\n",
    "\n",
    "    inputs, targets = batch\n",
    "    inputs = inputs.to(device)\n",
    "\n",
    "    batch_normalized = normalization(inputs.clone().detach())\n",
    "    with torch.no_grad():\n",
    "        logits = model(batch_normalized)\n",
    "\n",
    "    targets_sorted = torch.topk(logits, 1000)[1][0].cpu().numpy()\n",
    "    targets = np.concatenate((targets_sorted[:50], targets_sorted[50:950:20], targets_sorted[950:]))    \n",
    "\n",
    "    for target in tqdm(targets, desc=\"Processing targets...\"):\n",
    "        explanation = get_explanation(model, inputs, target, 'normal')\n",
    "        explanation_norm = torch.norm(explanation)\n",
    "        logit_explanation_norm.append((i, logits[0][target].cpu(), explanation_norm.cpu()))\n",
    "\n",
    "\n",
    "# Save results\n",
    "# Creating separate lists for x and y coordinates\n",
    "ids, x, y = zip(*logit_explanation_norm)\n",
    "\n",
    "ids = torch.tensor(ids).numpy()\n",
    "x = torch.tensor(x).numpy()\n",
    "y = torch.tensor(y).numpy()\n",
    "\n",
    "df = pd.DataFrame({'Logit Value': x, 'Gradient Norm': y, 'ID': ids})\n",
    "\n",
    "with open(\"test-id_logit_explanation_norm.pkl\", 'wb') as file:\n",
    "    pickle.dump(df, file)\n",
    "\n",
    "\n",
    "# Plotting with Seaborn\n",
    "sns.regplot(x='Logit Value', y='Gradient Norm', data=df, order=2, scatter=False, color='red')\n",
    "\n",
    "# Overlaying with scatterplot for colored data points\n",
    "sns.scatterplot(x='Logit Value', y='Gradient Norm', hue='ID', data=df, palette='viridis', legend=False)\n",
    "\n",
    "# Displaying the plot\n",
    "plt.savefig(\"logit_gradientnorm_relationship.png\", bbox_inches='tight', pad_inches=0)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-pytorch-pytorch",
   "name": "workbench-notebooks.m111",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m111"
  },
  "kernelspec": {
   "display_name": "PyTorch 1-13",
   "language": "python",
   "name": "conda-env-pytorch-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
